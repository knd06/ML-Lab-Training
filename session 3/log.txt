step: 1, loss: 4.619029998779297
step: 2, loss: 0.021856190636754036
step: 3, loss: 0.00021032284712418914
step: 4, loss: 1.647464500820206e-06
step: 5, loss: 8.34464515264699e-08
step: 6, loss: 0.0
step: 7, loss: 0.0
step: 8, loss: 0.0
step: 9, loss: 0.0
step: 10, loss: 10.915252685546875
step: 11, loss: 25.903100967407227
step: 12, loss: 22.24530029296875
step: 13, loss: 17.190824508666992
step: 14, loss: 11.935359954833984
step: 15, loss: 5.256526947021484
step: 16, loss: 0.9005930423736572
step: 17, loss: 0.11127452552318573
step: 18, loss: 0.00044471147703006864
step: 19, loss: 2.546222503951867e-06
step: 20, loss: 1.0967247021653748e-07
step: 21, loss: 0.0
step: 22, loss: 22.986347198486328
step: 23, loss: 30.151840209960938
step: 24, loss: 27.75531768798828
step: 25, loss: 24.287656784057617
step: 26, loss: 20.539175033569336
step: 27, loss: 16.591466903686523
step: 28, loss: 12.9168701171875
step: 29, loss: 9.433944702148438
step: 30, loss: 5.702805995941162
step: 31, loss: 2.7134804725646973
step: 32, loss: 0.5255370140075684
step: 33, loss: 0.18317027390003204
step: 34, loss: 16.20293617248535
step: 35, loss: 17.306425094604492
step: 36, loss: 16.391307830810547
step: 37, loss: 15.133990287780762
step: 38, loss: 13.169498443603516
step: 39, loss: 11.256970405578613
step: 40, loss: 9.85954475402832
step: 41, loss: 8.249763488769531
step: 42, loss: 7.060398101806641
step: 43, loss: 5.8788886070251465
step: 44, loss: 4.516226768493652
step: 45, loss: 3.6454017162323
step: 46, loss: 6.123903751373291
step: 47, loss: 5.313294887542725
step: 48, loss: 4.838501453399658
step: 49, loss: 4.047897815704346
step: 50, loss: 3.3285911083221436
step: 51, loss: 2.605581760406494
step: 52, loss: 2.02217173576355
step: 53, loss: 1.3559106588363647
step: 54, loss: 0.870269238948822
step: 55, loss: 0.5630631446838379
step: 56, loss: 0.29845839738845825
step: 57, loss: 6.643439769744873
step: 58, loss: 11.993871688842773
step: 59, loss: 11.92419719696045
step: 60, loss: 10.991893768310547
step: 61, loss: 10.03126049041748
step: 62, loss: 8.705450057983398
step: 63, loss: 7.629200458526611
step: 64, loss: 6.265539169311523
step: 65, loss: 5.156505107879639
step: 66, loss: 4.1706156730651855
step: 67, loss: 3.054218053817749
step: 68, loss: 2.337108850479126
step: 69, loss: 5.243954658508301
step: 70, loss: 5.8668622970581055
step: 71, loss: 4.896339416503906
step: 72, loss: 4.414757251739502
step: 73, loss: 3.47330904006958
step: 74, loss: 3.0667645931243896
step: 75, loss: 2.813392400741577
step: 76, loss: 2.4279704093933105
step: 77, loss: 2.0327365398406982
step: 78, loss: 1.8166638612747192
step: 79, loss: 1.3479936122894287
step: 80, loss: 1.1365128755569458
step: 81, loss: 6.011858940124512
step: 82, loss: 6.098832607269287
step: 83, loss: 5.489893913269043
step: 84, loss: 5.284559726715088
step: 85, loss: 4.96755838394165
step: 86, loss: 4.503702640533447
step: 87, loss: 4.31449031829834
step: 88, loss: 3.8531415462493896
step: 89, loss: 3.6565608978271484
step: 90, loss: 3.3548293113708496
step: 91, loss: 3.1028053760528564
step: 92, loss: 3.7635114192962646
step: 93, loss: 10.64111042022705
step: 94, loss: 10.196993827819824
step: 95, loss: 9.895088195800781
step: 96, loss: 9.504246711730957
step: 97, loss: 9.332356452941895
step: 98, loss: 8.920317649841309
step: 99, loss: 8.787836074829102
step: 100, loss: 8.505705833435059
step: 101, loss: 8.156561851501465
step: 102, loss: 7.931708812713623
step: 103, loss: 7.7247748374938965
step: 104, loss: 7.4084858894348145
step: 105, loss: 6.14080810546875
step: 106, loss: 5.919621467590332
step: 107, loss: 5.7923479080200195
step: 108, loss: 5.678657054901123
step: 109, loss: 5.241893291473389
step: 110, loss: 5.061581611633301
step: 111, loss: 4.710083484649658
step: 112, loss: 4.485875129699707
step: 113, loss: 4.228095531463623
step: 114, loss: 3.9265365600585938
step: 115, loss: 3.6753721237182617
step: 116, loss: 4.6906633377075195
step: 117, loss: 9.727523803710938
step: 118, loss: 9.542817115783691
step: 119, loss: 9.330665588378906
step: 120, loss: 9.132586479187012
step: 121, loss: 8.868523597717285
step: 122, loss: 8.663402557373047
step: 123, loss: 8.33730411529541
step: 124, loss: 8.084473609924316
step: 125, loss: 7.829742908477783
step: 126, loss: 7.581918239593506
step: 127, loss: 7.290554046630859
step: 128, loss: 7.4643778800964355
step: 129, loss: 8.970721244812012
step: 130, loss: 8.825821876525879
step: 131, loss: 8.635457038879395
step: 132, loss: 8.393662452697754
step: 133, loss: 8.156108856201172
step: 134, loss: 7.914855480194092
step: 135, loss: 7.660313606262207
step: 136, loss: 7.4449944496154785
step: 137, loss: 7.0984787940979
step: 138, loss: 6.811662673950195
step: 139, loss: 6.591086387634277
step: 140, loss: 6.2124152183532715
step: 141, loss: 5.909566879272461
step: 142, loss: 5.734409809112549
step: 143, loss: 5.5115861892700195
step: 144, loss: 5.276998996734619
step: 145, loss: 5.095552921295166
step: 146, loss: 4.784343719482422
step: 147, loss: 4.544768810272217
step: 148, loss: 4.291619777679443
step: 149, loss: 4.022549629211426
step: 150, loss: 3.7260327339172363
step: 151, loss: 3.4425504207611084
step: 152, loss: 3.739784002304077
step: 153, loss: 4.252312183380127
step: 154, loss: 4.079676628112793
step: 155, loss: 3.9294259548187256
step: 156, loss: 3.6889374256134033
step: 157, loss: 3.453540563583374
step: 158, loss: 3.292738437652588
step: 159, loss: 3.0067861080169678
step: 160, loss: 2.763282537460327
step: 161, loss: 2.513856887817383
step: 162, loss: 2.280993700027466
step: 163, loss: 2.0623226165771484
step: 164, loss: 6.3010663986206055
step: 165, loss: 9.27413558959961
step: 166, loss: 9.098730087280273
step: 167, loss: 8.929184913635254
step: 168, loss: 8.736666679382324
step: 169, loss: 8.531760215759277
step: 170, loss: 8.283843040466309
step: 171, loss: 8.039087295532227
step: 172, loss: 7.767721176147461
step: 173, loss: 7.496636390686035
step: 174, loss: 7.222958564758301
step: 175, loss: 6.8304290771484375
step: 176, loss: 9.139158248901367
step: 177, loss: 9.358071327209473
step: 178, loss: 9.128732681274414
step: 179, loss: 8.800834655761719
step: 180, loss: 8.496400833129883
step: 181, loss: 8.202851295471191
step: 182, loss: 7.821707725524902
step: 183, loss: 7.4939351081848145
step: 184, loss: 7.187504768371582
step: 185, loss: 6.8620452880859375
step: 186, loss: 6.550863742828369
step: 187, loss: 6.236903190612793
step: 188, loss: 7.222299098968506
step: 189, loss: 7.463400840759277
step: 190, loss: 7.249773979187012
step: 191, loss: 7.0316925048828125
step: 192, loss: 6.7269978523254395
step: 193, loss: 6.455740451812744
step: 194, loss: 6.3338727951049805
step: 195, loss: 5.842020034790039
step: 196, loss: 5.52053689956665
step: 197, loss: 5.1805853843688965
step: 198, loss: 4.839059352874756
step: 199, loss: 8.320808410644531
step: 200, loss: 8.872849464416504
step: 201, loss: 8.694169044494629
step: 202, loss: 8.39808177947998
step: 203, loss: 8.123022079467773
step: 204, loss: 7.857603549957275
step: 205, loss: 7.5134687423706055
step: 206, loss: 7.130388259887695
step: 207, loss: 6.698026180267334
step: 208, loss: 6.175049304962158
step: 209, loss: 5.657347202301025
step: 210, loss: 6.888042449951172
step: 211, loss: 7.85511589050293
step: 212, loss: 7.474173545837402
step: 213, loss: 7.0541181564331055
step: 214, loss: 6.64987850189209
step: 215, loss: 6.2160234451293945
step: 216, loss: 5.943211555480957
step: 217, loss: 5.719054698944092
step: 218, loss: 5.379634380340576
step: 219, loss: 5.521108627319336
step: 220, loss: 6.819131851196289
step: 221, loss: 6.627621650695801
step: 222, loss: 6.398391246795654
step: 223, loss: 6.0353169441223145
step: 224, loss: 5.796050548553467
step: 225, loss: 5.384722709655762
step: 226, loss: 2.8236398696899414
step: 227, loss: 2.6140685081481934
step: 228, loss: 3.0603866577148438
step: 229, loss: 2.9570324420928955
step: 230, loss: 2.9109363555908203
step: 231, loss: 2.824589252471924
step: 232, loss: 2.9610636234283447
step: 233, loss: 2.6680192947387695
step: 234, loss: 2.7894093990325928
step: 235, loss: 2.835458993911743
step: 236, loss: 2.734100580215454
step: 237, loss: 2.5711662769317627
step: 238, loss: 2.7251975536346436
step: 239, loss: 2.637155771255493
step: 240, loss: 2.8176369667053223
step: 241, loss: 2.681490182876587
step: 242, loss: 2.310631036758423
step: 243, loss: 2.3637914657592773
step: 244, loss: 2.5105512142181396
step: 245, loss: 2.49150013923645
step: 246, loss: 2.119947910308838
step: 247, loss: 2.2220404148101807
step: 248, loss: 2.5063059329986572
step: 249, loss: 2.7621448040008545
step: 250, loss: 2.0769519805908203
step: 251, loss: 2.0628981590270996
step: 252, loss: 2.1634175777435303
step: 253, loss: 1.8671809434890747
step: 254, loss: 1.7554881572723389
step: 255, loss: 1.952117919921875
step: 256, loss: 1.9080173969268799
step: 257, loss: 2.2369027137756348
step: 258, loss: 1.9978783130645752
step: 259, loss: 1.9074915647506714
step: 260, loss: 2.260673761367798
step: 261, loss: 2.3611021041870117
step: 262, loss: 1.6099854707717896
step: 263, loss: 2.096710681915283
step: 264, loss: 1.7096319198608398
step: 265, loss: 1.6993180513381958
step: 266, loss: 1.6990448236465454
step: 267, loss: 1.8549256324768066
step: 268, loss: 1.7835415601730347
step: 269, loss: 1.7208296060562134
step: 270, loss: 1.5432969331741333
step: 271, loss: 1.3072738647460938
step: 272, loss: 1.6133686304092407
step: 273, loss: 1.8679423332214355
step: 274, loss: 1.6469697952270508
step: 275, loss: 1.605120062828064
step: 276, loss: 1.4813623428344727
step: 277, loss: 1.6700925827026367
step: 278, loss: 1.4212675094604492
step: 279, loss: 1.0338454246520996
step: 280, loss: 1.7755509614944458
step: 281, loss: 1.4059165716171265
step: 282, loss: 1.3069638013839722
step: 283, loss: 1.554802656173706
step: 284, loss: 1.2517123222351074
step: 285, loss: 1.1626684665679932
step: 286, loss: 1.1787878274917603
step: 287, loss: 1.1801018714904785
step: 288, loss: 1.4060460329055786
step: 289, loss: 1.5016255378723145
step: 290, loss: 1.3527336120605469
step: 291, loss: 1.308353066444397
step: 292, loss: 1.1285816431045532
step: 293, loss: 1.4088019132614136
step: 294, loss: 1.3779842853546143
step: 295, loss: 1.0575355291366577
step: 296, loss: 1.2683310508728027
step: 297, loss: 0.9738850593566895
step: 298, loss: 1.4124093055725098
step: 299, loss: 1.1146306991577148
step: 300, loss: 0.6515108346939087
step: 301, loss: 0.9598844051361084
step: 302, loss: 1.0414042472839355
step: 303, loss: 1.121545433998108
step: 304, loss: 1.2727293968200684
step: 305, loss: 0.9081123471260071
step: 306, loss: 1.1352962255477905
step: 307, loss: 1.2463908195495605
step: 308, loss: 0.9776245951652527
step: 309, loss: 0.9137775301933289
step: 310, loss: 0.9766985177993774
step: 311, loss: 0.5253207683563232
step: 312, loss: 0.6468988060951233
step: 313, loss: 0.797059178352356
step: 314, loss: 0.7738737463951111
step: 315, loss: 1.2864171266555786
step: 316, loss: 1.3449044227600098
step: 317, loss: 0.9144130945205688
step: 318, loss: 0.8401167392730713
step: 319, loss: 0.7086548805236816
step: 320, loss: 0.7670850157737732
step: 321, loss: 1.049850583076477
step: 322, loss: 0.8526087403297424
step: 323, loss: 1.0520319938659668
step: 324, loss: 0.48830729722976685
step: 325, loss: 0.5503535866737366
step: 326, loss: 0.9931042194366455
step: 327, loss: 1.069657325744629
step: 328, loss: 0.5470743775367737
step: 329, loss: 0.6520535349845886
step: 330, loss: 1.2487177848815918
step: 331, loss: 0.5181438326835632
step: 332, loss: 0.8682430386543274
step: 333, loss: 0.8408477306365967
step: 334, loss: 0.6932871341705322
step: 335, loss: 0.5511974096298218
step: 336, loss: 0.6527247428894043
step: 337, loss: 0.5335116386413574
step: 338, loss: 0.5035094618797302
step: 339, loss: 0.6626891493797302
step: 340, loss: 0.4726446568965912
step: 341, loss: 0.6376469135284424
step: 342, loss: 0.7759202718734741
step: 343, loss: 0.553054928779602
step: 344, loss: 0.6948440074920654
step: 345, loss: 0.3433573246002197
step: 346, loss: 0.5237081050872803
step: 347, loss: 0.705503523349762
step: 348, loss: 0.7169066667556763
step: 349, loss: 0.6851784586906433
step: 350, loss: 0.4050039052963257
step: 351, loss: 0.51957106590271
step: 352, loss: 0.5967894792556763
step: 353, loss: 0.7446821331977844
step: 354, loss: 0.38432350754737854
step: 355, loss: 0.7507964372634888
step: 356, loss: 0.47611281275749207
step: 357, loss: 0.4294954240322113
step: 358, loss: 0.5643692016601562
step: 359, loss: 0.24952226877212524
step: 360, loss: 0.3943563401699066
step: 361, loss: 0.7841973900794983
step: 362, loss: 0.3189617395401001
step: 363, loss: 0.4937663972377777
step: 364, loss: 0.39501118659973145
step: 365, loss: 0.6044589281082153
step: 366, loss: 0.39598754048347473
step: 367, loss: 0.2132248431444168
step: 368, loss: 0.3514614999294281
step: 369, loss: 0.5837223529815674
step: 370, loss: 0.3049221932888031
step: 371, loss: 0.43994349241256714
step: 372, loss: 0.06759876012802124
step: 373, loss: 0.4143476188182831
step: 374, loss: 0.7599661946296692
step: 375, loss: 0.3875865042209625
step: 376, loss: 0.4532131254673004
step: 377, loss: 0.2605850398540497
step: 378, loss: 0.7572776675224304
step: 379, loss: 1.0753062963485718
step: 380, loss: 0.5698935985565186
step: 381, loss: 0.4995165169239044
step: 382, loss: 0.4394708275794983
step: 383, loss: 0.5503007769584656
step: 384, loss: 0.4609414339065552
step: 385, loss: 0.6133356690406799
step: 386, loss: 0.7894076704978943
step: 387, loss: 0.3672383427619934
step: 388, loss: 0.6752507090568542
step: 389, loss: 0.486697793006897
step: 390, loss: 0.414450079202652
step: 391, loss: 0.7687318921089172
step: 392, loss: 0.3721984028816223
step: 393, loss: 0.29383349418640137
step: 394, loss: 0.21744705736637115
step: 395, loss: 0.37832123041152954
step: 396, loss: 0.5851669907569885
step: 397, loss: 0.5060151219367981
step: 398, loss: 0.36387383937835693
step: 399, loss: 0.3367922902107239
step: 400, loss: 0.3962155282497406
step: 401, loss: 0.5178619027137756
step: 402, loss: 0.6566766500473022
step: 403, loss: 0.2676314413547516
step: 404, loss: 0.6747952103614807
step: 405, loss: 0.2922370433807373
step: 406, loss: 0.34019896388053894
step: 407, loss: 0.2661948800086975
step: 408, loss: 0.18460948765277863
step: 409, loss: 0.8200756311416626
step: 410, loss: 0.2011275291442871
step: 411, loss: 0.4434811770915985
step: 412, loss: 0.34309685230255127
step: 413, loss: 0.2766859531402588
step: 414, loss: 0.5634105205535889
step: 415, loss: 0.469176322221756
step: 416, loss: 0.5662164092063904
step: 417, loss: 0.35137173533439636
step: 418, loss: 0.5048481822013855
step: 419, loss: 0.4593122899532318
step: 420, loss: 0.2803424596786499
step: 421, loss: 0.309359610080719
step: 422, loss: 0.5617232322692871
step: 423, loss: 0.35326695442199707
step: 424, loss: 0.6893799304962158
step: 425, loss: 0.35962051153182983
step: 426, loss: 0.3839690089225769
step: 427, loss: 0.510566234588623
step: 428, loss: 0.14007002115249634
step: 429, loss: 0.42510098218917847
step: 430, loss: 0.6458410024642944
step: 431, loss: 0.5296280384063721
step: 432, loss: 0.7102163434028625
step: 433, loss: 0.24467378854751587
step: 434, loss: 0.3126845955848694
step: 435, loss: 0.4826546013355255
step: 436, loss: 0.47761422395706177
step: 437, loss: 0.3740028440952301
step: 438, loss: 0.19664061069488525
step: 439, loss: 0.27209460735321045
step: 440, loss: 0.7063336968421936
step: 441, loss: 0.6561458706855774
step: 442, loss: 0.383350670337677
step: 443, loss: 0.33892887830734253
step: 444, loss: 0.47334393858909607
step: 445, loss: 0.6068227887153625
step: 446, loss: 0.21168434619903564
step: 447, loss: 0.2715661823749542
step: 448, loss: 0.29887768626213074
step: 449, loss: 0.31867289543151855
step: 450, loss: 0.4870002865791321
step: 451, loss: 0.2776297926902771
step: 452, loss: 0.08551757037639618
step: 453, loss: 0.3130970597267151
step: 454, loss: 0.013310544192790985
step: 455, loss: 0.10777661949396133
step: 456, loss: 0.5443294048309326
step: 457, loss: 0.025521617382764816
step: 458, loss: 0.0507100448012352
step: 459, loss: 0.09010583162307739
step: 460, loss: 0.03027421422302723
step: 461, loss: 0.05706976354122162
step: 462, loss: 0.39473795890808105
step: 463, loss: 0.09866158664226532
step: 464, loss: 0.0774393379688263
step: 465, loss: 0.1598610281944275
step: 466, loss: 0.0721559152007103
step: 467, loss: 0.17340518534183502
step: 468, loss: 0.08164635300636292
step: 469, loss: 0.0902535542845726
step: 470, loss: 0.0840168371796608
step: 471, loss: 0.20061594247817993
step: 472, loss: 0.04742307960987091
step: 473, loss: 0.14045239984989166
step: 474, loss: 0.028830768540501595
step: 475, loss: 0.11769723892211914
step: 476, loss: 0.013593792915344238
step: 477, loss: 0.09677043557167053
step: 478, loss: 0.07556862384080887
step: 479, loss: 0.13754335045814514
step: 480, loss: 0.16529937088489532
step: 481, loss: 0.3559294044971466
step: 482, loss: 0.07869081199169159
step: 483, loss: 0.06129181757569313
step: 484, loss: 0.22422870993614197
step: 485, loss: 0.18944142758846283
step: 486, loss: 0.14736142754554749
step: 487, loss: 0.37513598799705505
step: 488, loss: 0.0791349858045578
step: 489, loss: 0.1955251693725586
step: 490, loss: 0.30896273255348206
step: 491, loss: 0.046635694801807404
step: 492, loss: 0.26212507486343384
step: 493, loss: 0.12098509818315506
step: 494, loss: 0.17835687100887299
step: 495, loss: 0.21036432683467865
step: 496, loss: 0.035810764878988266
step: 497, loss: 0.10555621981620789
step: 498, loss: 0.18178138136863708
step: 499, loss: 0.15496288239955902
step: 500, loss: 0.08495371788740158
step: 501, loss: 0.09976457804441452
step: 502, loss: 0.11297518014907837
step: 503, loss: 0.31864434480667114
step: 504, loss: 0.14917708933353424
step: 505, loss: 0.06026694178581238
step: 506, loss: 0.12563791871070862
step: 507, loss: 0.07766693830490112
step: 508, loss: 0.10652819275856018
step: 509, loss: 0.2995193898677826
step: 510, loss: 0.060237154364585876
step: 511, loss: 0.28780126571655273
step: 512, loss: 0.15981286764144897
step: 513, loss: 0.3091590404510498
step: 514, loss: 0.3375805616378784
step: 515, loss: 0.07639855891466141
step: 516, loss: 0.35998889803886414
step: 517, loss: 0.23127132654190063
step: 518, loss: 0.011825747787952423
step: 519, loss: 0.09986568242311478
step: 520, loss: 0.22827789187431335
step: 521, loss: 0.25613346695899963
step: 522, loss: 0.06817889958620071
step: 523, loss: 0.08784894645214081
step: 524, loss: 0.10025998204946518
step: 525, loss: 0.15644782781600952
step: 526, loss: 0.08750750869512558
step: 527, loss: 0.0970149040222168
step: 528, loss: 0.18169020116329193
step: 529, loss: 0.04130499064922333
step: 530, loss: 0.02789054438471794
step: 531, loss: 0.3577931225299835
step: 532, loss: 0.143192857503891
step: 533, loss: 0.09814145416021347
step: 534, loss: 0.055933669209480286
step: 535, loss: 0.2549860179424286
step: 536, loss: 0.02980971895158291
step: 537, loss: 0.27124157547950745
step: 538, loss: 0.12103316932916641
step: 539, loss: 0.07509937882423401
step: 540, loss: 0.16372844576835632
step: 541, loss: 0.398215115070343
step: 542, loss: 0.20484821498394012
step: 543, loss: 0.24054092168807983
step: 544, loss: 0.058402031660079956
step: 545, loss: 0.11912365257740021
step: 546, loss: 0.19366784393787384
step: 547, loss: 0.12243635207414627
step: 548, loss: 0.2644723951816559
step: 549, loss: 0.18358060717582703
step: 550, loss: 0.023342661559581757
step: 551, loss: 0.2309255748987198
step: 552, loss: 0.41933348774909973
step: 553, loss: 0.11715593189001083
step: 554, loss: 0.15682892501354218
step: 555, loss: 0.17612171173095703
step: 556, loss: 0.08448950946331024
step: 557, loss: 0.07419443130493164
step: 558, loss: 0.08802290260791779
step: 559, loss: 0.03818906098604202
step: 560, loss: 0.013649660162627697
step: 561, loss: 0.18445633351802826
step: 562, loss: 0.22944428026676178
step: 563, loss: 0.0908673107624054
step: 564, loss: 0.18574558198451996
step: 565, loss: 0.1211266815662384
step: 566, loss: 0.5210725665092468
step: 567, loss: 0.24858704209327698
step: 568, loss: 0.1597137451171875
step: 569, loss: 0.14780335128307343
step: 570, loss: 0.45972275733947754
step: 571, loss: 0.03176135942339897
step: 572, loss: 0.19153179228305817
step: 573, loss: 0.08772221207618713
step: 574, loss: 0.04982145130634308
step: 575, loss: 0.13681653141975403
step: 576, loss: 0.17640714347362518
step: 577, loss: 0.07786636054515839
step: 578, loss: 0.0671679675579071
step: 579, loss: 0.16221992671489716
step: 580, loss: 0.18214036524295807
step: 581, loss: 0.11034642159938812
step: 582, loss: 0.14314350485801697
step: 583, loss: 0.031573258340358734
step: 584, loss: 0.07402477413415909
step: 585, loss: 0.014573236927390099
step: 586, loss: 0.043425291776657104
step: 587, loss: 0.44215476512908936
step: 588, loss: 0.120852530002594
step: 589, loss: 0.22852355241775513
step: 590, loss: 0.2878119945526123
step: 591, loss: 0.17610426247119904
step: 592, loss: 0.1272972822189331
step: 593, loss: 0.04751631245017052
step: 594, loss: 0.05932023376226425
step: 595, loss: 0.051413848996162415
step: 596, loss: 0.42971351742744446
step: 597, loss: 0.11371608078479767
step: 598, loss: 0.12192992120981216
step: 599, loss: 0.01241803728044033
step: 600, loss: 0.019061481580138206
step: 601, loss: 0.14323493838310242
step: 602, loss: 0.07419221848249435
step: 603, loss: 0.12976981699466705
step: 604, loss: 0.06108902767300606
step: 605, loss: 0.22872528433799744
step: 606, loss: 0.20945508778095245
step: 607, loss: 0.02840885892510414
step: 608, loss: 0.05485682561993599
step: 609, loss: 0.21793638169765472
step: 610, loss: 0.03886111080646515
step: 611, loss: 0.12727975845336914
step: 612, loss: 0.21030549705028534
step: 613, loss: 0.13608872890472412
step: 614, loss: 0.2872001528739929
step: 615, loss: 0.08645987510681152
step: 616, loss: 0.526986300945282
step: 617, loss: 0.4095103442668915
step: 618, loss: 0.07877623289823532
step: 619, loss: 0.22491340339183807
step: 620, loss: 0.2412402331829071
step: 621, loss: 0.04856184497475624
step: 622, loss: 0.19117456674575806
step: 623, loss: 0.16581939160823822
step: 624, loss: 0.0762949287891388
step: 625, loss: 0.2813118100166321
step: 626, loss: 0.10417253524065018
step: 627, loss: 0.07899738103151321
step: 628, loss: 0.264396607875824
step: 629, loss: 0.24970699846744537
step: 630, loss: 0.1627332717180252
step: 631, loss: 0.13383501768112183
step: 632, loss: 0.0709473118185997
step: 633, loss: 0.23214071989059448
step: 634, loss: 0.11301395297050476
step: 635, loss: 0.14781829714775085
step: 636, loss: 0.19911016523838043
step: 637, loss: 0.11051398515701294
step: 638, loss: 0.18374918401241302
step: 639, loss: 0.03762572631239891
step: 640, loss: 0.12153546512126923
step: 641, loss: 0.21198442578315735
step: 642, loss: 0.010616904124617577
step: 643, loss: 0.012331834994256496
step: 644, loss: 0.11734791845083237
step: 645, loss: 0.18922996520996094
step: 646, loss: 0.2581464648246765
step: 647, loss: 0.043609198182821274
step: 648, loss: 0.34683284163475037
step: 649, loss: 0.03064105473458767
step: 650, loss: 0.07242602109909058
step: 651, loss: 0.04282045736908913
step: 652, loss: 0.1563485860824585
step: 653, loss: 0.1650247424840927
step: 654, loss: 0.0060310568660497665
step: 655, loss: 0.15802524983882904
step: 656, loss: 0.19547277688980103
step: 657, loss: 0.13107441365718842
step: 658, loss: 0.13592970371246338
step: 659, loss: 0.27787935733795166
step: 660, loss: 0.1286521852016449
step: 661, loss: 0.2534928619861603
step: 662, loss: 0.2686815857887268
step: 663, loss: 0.09765525907278061
step: 664, loss: 0.1066206842660904
step: 665, loss: 0.22014230489730835
step: 666, loss: 0.0614018589258194
step: 667, loss: 0.11321401596069336
step: 668, loss: 0.36125198006629944
step: 669, loss: 0.2429257184267044
step: 670, loss: 0.2022060602903366
step: 671, loss: 0.29209890961647034
step: 672, loss: 0.17839501798152924
step: 673, loss: 0.4141632914543152
step: 674, loss: 0.19404171407222748
step: 675, loss: 0.00920861680060625
step: 676, loss: 0.2301846444606781
step: 677, loss: 0.017420446500182152
step: 678, loss: 0.007152467966079712
step: 679, loss: 0.21151673793792725
step: 680, loss: 0.07657598704099655
step: 681, loss: 0.024442559108138084
step: 682, loss: 0.020700201392173767
step: 683, loss: 0.05921877920627594
step: 684, loss: 0.022394375875592232
step: 685, loss: 0.10065077990293503
step: 686, loss: 0.00941016897559166
step: 687, loss: 0.02434181421995163
step: 688, loss: 0.018579797819256783
step: 689, loss: 0.02790856920182705
step: 690, loss: 0.07978031784296036
step: 691, loss: 0.028271982446312904
step: 692, loss: 0.011186442337930202
step: 693, loss: 0.041068095713853836
step: 694, loss: 0.1273084282875061
step: 695, loss: 0.00880554411560297
step: 696, loss: 0.1034831702709198
step: 697, loss: 0.004115995950996876
step: 698, loss: 0.04649103805422783
step: 699, loss: 0.004489285405725241
step: 700, loss: 0.0027123636100441217
step: 701, loss: 0.003530366811901331
step: 702, loss: 0.14677338302135468
step: 703, loss: 0.015189413912594318
step: 704, loss: 0.004534805193543434
step: 705, loss: 0.020082267001271248
step: 706, loss: 0.10906697064638138
step: 707, loss: 0.031526029109954834
step: 708, loss: 0.02639152854681015
step: 709, loss: 0.014728388749063015
step: 710, loss: 0.004469393752515316
step: 711, loss: 0.11870989203453064
step: 712, loss: 0.07240951061248779
step: 713, loss: 0.013638516888022423
step: 714, loss: 0.024699566885828972
step: 715, loss: 0.008408068679273129
step: 716, loss: 0.024253850802779198
step: 717, loss: 0.025984540581703186
step: 718, loss: 0.009531762450933456
step: 719, loss: 0.005206095054745674
step: 720, loss: 0.05774476006627083
step: 721, loss: 0.009426414966583252
step: 722, loss: 0.042599476873874664
step: 723, loss: 0.022984804585576057
step: 724, loss: 0.08030057698488235
step: 725, loss: 0.056366387754678726
step: 726, loss: 0.02297907881438732
step: 727, loss: 0.008639911189675331
step: 728, loss: 0.027432026341557503
step: 729, loss: 0.0066975480876863
step: 730, loss: 0.0504441037774086
step: 731, loss: 0.02055954933166504
step: 732, loss: 0.003822101280093193
step: 733, loss: 0.18700020015239716
step: 734, loss: 0.03918416425585747
step: 735, loss: 0.044182587414979935
step: 736, loss: 0.14563734829425812
step: 737, loss: 0.03308374807238579
step: 738, loss: 0.02094363234937191
step: 739, loss: 0.03223894536495209
step: 740, loss: 0.25441399216651917
step: 741, loss: 0.14600862562656403
step: 742, loss: 0.004419662524014711
step: 743, loss: 0.0032144964206963778
step: 744, loss: 0.05552058294415474
step: 745, loss: 0.03231477737426758
step: 746, loss: 0.026911472901701927
step: 747, loss: 0.0022861319594085217
step: 748, loss: 0.04988185316324234
step: 749, loss: 0.008623925037682056
step: 750, loss: 0.01091788336634636
step: 751, loss: 0.004658220801502466
step: 752, loss: 0.07674706727266312
step: 753, loss: 0.004921596497297287
step: 754, loss: 0.005134871695190668
step: 755, loss: 0.032171253114938736
step: 756, loss: 0.10376045107841492
step: 757, loss: 0.01846352219581604
step: 758, loss: 0.002318137791007757
step: 759, loss: 0.04541584104299545
step: 760, loss: 0.01762525364756584
step: 761, loss: 0.15803290903568268
step: 762, loss: 0.00730109354481101
step: 763, loss: 0.030311038717627525
step: 764, loss: 0.032309044152498245
step: 765, loss: 0.019007140770554543
step: 766, loss: 0.07130458205938339
step: 767, loss: 0.15813200175762177
step: 768, loss: 0.021412443369627
step: 769, loss: 0.061726413667201996
step: 770, loss: 0.04041771963238716
step: 771, loss: 0.053657837212085724
step: 772, loss: 0.015663081780076027
step: 773, loss: 0.0020762349013239145
step: 774, loss: 0.022041788324713707
step: 775, loss: 0.004529392812401056
step: 776, loss: 0.013146850280463696
step: 777, loss: 0.0035637489054352045
step: 778, loss: 0.0073873563669621944
step: 779, loss: 0.1353328824043274
step: 780, loss: 0.005749056115746498
step: 781, loss: 0.004158832598477602
step: 782, loss: 0.008461900055408478
step: 783, loss: 0.005723342765122652
step: 784, loss: 0.04660452902317047
step: 785, loss: 0.035363927483558655
step: 786, loss: 0.04981856420636177
step: 787, loss: 0.002756059169769287
step: 788, loss: 0.016927892342209816
step: 789, loss: 0.0013345968909561634
step: 790, loss: 0.010736072435975075
step: 791, loss: 0.0034678983502089977
step: 792, loss: 0.010871807113289833
step: 793, loss: 0.034377310425043106
step: 794, loss: 0.0032473746687173843
step: 795, loss: 0.09457335621118546
step: 796, loss: 0.005406196694821119
step: 797, loss: 0.006376076955348253
step: 798, loss: 0.03617222234606743
step: 799, loss: 0.008166111074388027
step: 800, loss: 0.00850747711956501
step: 801, loss: 0.008589141070842743
step: 802, loss: 0.03079584427177906
step: 803, loss: 0.0030239024199545383
step: 804, loss: 0.004973080009222031
step: 805, loss: 0.0032633959781378508
step: 806, loss: 0.004577456507831812
step: 807, loss: 0.03430231288075447
step: 808, loss: 0.002848768839612603
step: 809, loss: 0.004641076549887657
step: 810, loss: 0.07704096287488937
step: 811, loss: 0.0025872373953461647
step: 812, loss: 0.03415055200457573
step: 813, loss: 0.036696769297122955
step: 814, loss: 0.002440042793750763
step: 815, loss: 0.03149418532848358
step: 816, loss: 0.0033486983738839626
step: 817, loss: 0.00798536092042923
step: 818, loss: 0.0024549064692109823
step: 819, loss: 0.0330343171954155
step: 820, loss: 0.0054123722948133945
step: 821, loss: 0.0012664432870224118
step: 822, loss: 0.004060964100062847
step: 823, loss: 0.005400361027568579
step: 824, loss: 0.002599436091259122
step: 825, loss: 0.029755974188447
step: 826, loss: 0.04569171369075775
step: 827, loss: 0.001266776816919446
step: 828, loss: 0.002369011053815484
step: 829, loss: 0.011254712007939816
step: 830, loss: 0.003530976828187704
step: 831, loss: 0.011492223478853703
step: 832, loss: 0.004346954170614481
step: 833, loss: 0.017559148371219635
step: 834, loss: 0.010268268175423145
step: 835, loss: 0.011253362521529198
step: 836, loss: 0.00601464556530118
step: 837, loss: 0.012865228578448296
step: 838, loss: 0.039456695318222046
step: 839, loss: 0.04349414259195328
step: 840, loss: 0.04743659496307373
step: 841, loss: 0.041357606649398804
step: 842, loss: 0.02059144526720047
step: 843, loss: 0.0027075333055108786
step: 844, loss: 0.07257437705993652
step: 845, loss: 0.0011698793387040496
step: 846, loss: 0.005329423118382692
step: 847, loss: 0.002686241175979376
step: 848, loss: 0.005543425679206848
step: 849, loss: 0.018654366955161095
step: 850, loss: 0.011473758146166801
step: 851, loss: 0.01630144566297531
step: 852, loss: 0.0035060413647443056
step: 853, loss: 0.10341650992631912
step: 854, loss: 0.11269480735063553
step: 855, loss: 0.038830555975437164
step: 856, loss: 0.005367865785956383
step: 857, loss: 0.005094768945127726
step: 858, loss: 0.02321096509695053
step: 859, loss: 0.01711871661245823
step: 860, loss: 0.022244861349463463
step: 861, loss: 0.004171324893832207
step: 862, loss: 0.024938039481639862
step: 863, loss: 0.004878902807831764
step: 864, loss: 0.03674962371587753
step: 865, loss: 0.13337475061416626
step: 866, loss: 0.02369130402803421
step: 867, loss: 0.006308578420430422
step: 868, loss: 0.06300201267004013
step: 869, loss: 0.008699466474354267
step: 870, loss: 0.012666319496929646
step: 871, loss: 0.009712317027151585
step: 872, loss: 0.001938130590133369
step: 873, loss: 0.07766823470592499
step: 874, loss: 0.04431817680597305
step: 875, loss: 0.006850657984614372
step: 876, loss: 0.047083500772714615
step: 877, loss: 0.004330789670348167
step: 878, loss: 0.03939986601471901
step: 879, loss: 0.002717836294323206
step: 880, loss: 0.005826619453728199
step: 881, loss: 0.004504833370447159
step: 882, loss: 0.04074704647064209
step: 883, loss: 0.019706403836607933
step: 884, loss: 0.007354519329965115
step: 885, loss: 0.005669042933732271
step: 886, loss: 0.018310338258743286
step: 887, loss: 0.04336624592542648
step: 888, loss: 0.0027987021021544933
step: 889, loss: 0.0070155407302081585
step: 890, loss: 0.018881697207689285
step: 891, loss: 0.10301478207111359
step: 892, loss: 0.002585620852187276
step: 893, loss: 0.013681506738066673
step: 894, loss: 0.007591866888105869
step: 895, loss: 0.09207036346197128
step: 896, loss: 0.013520471751689911
step: 897, loss: 0.07550966739654541
step: 898, loss: 0.00838605035096407
step: 899, loss: 0.1319071501493454
step: 900, loss: 0.006897951476275921
step: 901, loss: 0.0025020823813974857
step: 902, loss: 0.0082126809284091
step: 903, loss: 0.001167256268672645
step: 904, loss: 0.0012060535373166203
step: 905, loss: 0.005003480706363916
step: 906, loss: 0.024771470576524734
step: 907, loss: 0.0054465108551084995
step: 908, loss: 0.0017729145474731922
step: 909, loss: 0.002725868485867977
step: 910, loss: 0.0009541402105242014
step: 911, loss: 0.0032091056928038597
step: 912, loss: 0.0034417181741446257
step: 913, loss: 0.0025438403245061636
step: 914, loss: 0.0016098327469080687
step: 915, loss: 0.003196016186848283
step: 916, loss: 0.006313089746981859
step: 917, loss: 0.006677089259028435
step: 918, loss: 0.001789044588804245
step: 919, loss: 0.011274661868810654
step: 920, loss: 0.001657362561672926
step: 921, loss: 0.001571777742356062
step: 922, loss: 0.006438857410103083
step: 923, loss: 0.041145987808704376
step: 924, loss: 0.0015026218025013804
step: 925, loss: 0.000948344124481082
step: 926, loss: 0.04689750820398331
step: 927, loss: 0.002057400532066822
step: 928, loss: 0.001521360594779253
step: 929, loss: 0.03575551137328148
step: 930, loss: 0.0014840669464319944
step: 931, loss: 0.0018427801551297307
step: 932, loss: 0.010244075208902359
step: 933, loss: 0.003648508107289672
step: 934, loss: 0.005253995768725872
step: 935, loss: 0.0009430543286725879
step: 936, loss: 0.0012321383692324162
step: 937, loss: 0.006003286223858595
step: 938, loss: 0.0020633218809962273
step: 939, loss: 0.0014218840515241027
step: 940, loss: 0.0013795840786769986
step: 941, loss: 0.00557709950953722
step: 942, loss: 0.001083942479453981
step: 943, loss: 0.0055230408906936646
step: 944, loss: 0.002862878842279315
step: 945, loss: 0.0017149820923805237
step: 946, loss: 0.004151300992816687
step: 947, loss: 0.0014160036807879806
step: 948, loss: 0.0008460419485345483
step: 949, loss: 0.000724727869965136
step: 950, loss: 0.0007059242343530059
step: 951, loss: 0.008264766074717045
step: 952, loss: 0.0012022571172565222
step: 953, loss: 0.0007306752959266305
step: 954, loss: 0.000723868899513036
step: 955, loss: 0.0024754225742071867
step: 956, loss: 0.0016636006766930223
step: 957, loss: 0.007262700237333775
step: 958, loss: 0.04691534489393234
step: 959, loss: 0.00125607056543231
step: 960, loss: 0.00796595960855484
step: 961, loss: 0.003828509245067835
step: 962, loss: 0.0008759732008911669
step: 963, loss: 0.004370728973299265
step: 964, loss: 0.0047493744641542435
step: 965, loss: 0.014837379567325115
step: 966, loss: 0.000970731838606298
step: 967, loss: 0.0012888911878690124
step: 968, loss: 0.0016720144776627421
step: 969, loss: 0.0016397000290453434
step: 970, loss: 0.012630768120288849
step: 971, loss: 0.0011259192833676934
step: 972, loss: 0.0025527465622872114
step: 973, loss: 0.05490100756287575
step: 974, loss: 0.0013997609494253993
step: 975, loss: 0.04205165058374405
step: 976, loss: 0.003134001512080431
step: 977, loss: 0.03503929078578949
step: 978, loss: 0.0011698876041918993
step: 979, loss: 0.0005635020206682384
step: 980, loss: 0.000958385004196316
step: 981, loss: 0.002701375400647521
step: 982, loss: 0.0336042195558548
step: 983, loss: 0.016233496367931366
step: 984, loss: 0.06920094788074493
step: 985, loss: 0.10823441296815872
step: 986, loss: 0.001367876073345542
step: 987, loss: 0.0010792729444801807
step: 988, loss: 0.001687790616415441
step: 989, loss: 0.0008835741318762302
step: 990, loss: 0.002251393860206008
step: 991, loss: 0.010204840451478958
step: 992, loss: 0.0015990638639777899
step: 993, loss: 0.0004198510432615876
step: 994, loss: 0.00915908720344305
step: 995, loss: 0.010601136833429337
step: 996, loss: 0.1123175173997879
step: 997, loss: 0.0006052775424905121
step: 998, loss: 0.0019032610580325127
step: 999, loss: 0.00403512641787529
step: 1000, loss: 0.0014833235181868076
Epoch: 4
Accuracy on test data: 0.7877057886351566
